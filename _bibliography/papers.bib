---
---

@misc{lu2024disguised,
      abbr = {ICML 2024},
      title={Disguised Copyright Infringement of Latent Diffusion Models}, 
      author={Yiwei Lu* and Matthew Y. R. Yang* and Zuoqiu Liu* and Gautam Kamath and Yaoliang Yu},
      year={2024},
      eprint={2404.06737},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lu2024indiscriminate,
      abbr = {SaTML 2024},
      title={Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors}, 
      author={Yiwei Lu and Matthew Y. R. Yang and Gautam Kamath and Yaoliang Yu},
      year={2024},
      eprint={2402.12626},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{jiang-etal-2023-low,
    abbr = {ACL 2023},
    title = "{``}Low-Resource{''} Text Classification: A Parameter-Free Classification Method with Compressors",
    author = "Jiang, Zhiying  and
      Yang, Matthew Y. R. and
      Tsirlin, Mikhail  and
      Tang, Raphael  and
      Dai, Yiqin  and
      Lin, Jimmy",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.426",
    doi = "10.18653/v1/2023.findings-acl.426",
    pages = "6810--6828",
    abstract = "Deep neural networks (DNNs) are often used for text classification due to their high accuracy. However, DNNs can be computationally intensive, requiring millions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize, and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that{'}s easy, lightweight, and universal in text classification: a combination of a simple compressor like \textit{gzip} with a $k$-nearest-neighbor classifier. Without any training parameters, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distribution datasets.It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also excels in the few-shot setting, where labeled data are too scarce to train DNNs effectively.",
}

@inproceedings{10.1145/3529372.3533280,
abbr = {JCDL 2022},
author = {Yang, Matthew Y. R. and Yang, Siwen and Lin, Jimmy},
title = {Integration of text and geospatial search for hydrographic datasets using the lucene search library},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3533280},
doi = {10.1145/3529372.3533280},
abstract = {We present a hybrid text and geospatial search application for hydrographic datasets built on the open-source Lucene search library. Our goal is to demonstrate that it is possible to build custom GIS applications by integrating existing open-source components and data sources, which contrasts with existing approaches based on monolithic platforms such as ArcGIS and QGIS. Lucene provides rich index structures and search capabilities for free text and geometries; the former has already been integrated and exposed via our group's Anserini and Pyserini IR toolkits. In this work, we extend these toolkits to include geospatial capabilities. Combining knowledge extracted from Wikidata with the HydroSHEDS dataset, our application enables text and geospatial search of rivers worldwide.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {36},
numpages = {5},
keywords = {geospatial indexing, lucene, river basins, rivers},
location = {Cologne, Germany},
series = {JCDL '22}
}
